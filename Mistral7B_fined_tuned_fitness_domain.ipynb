{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOFIYEB/Blogging_with_AI_Text_Image_Generation/blob/main/Mistral7B_fined_tuned_fitness_domain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2Ofi_7AZVVuF"
      },
      "cell_type": "markdown",
      "source": [
        "# **FINE- TUNING MISTRAL 7B FOR A BLOG POST**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Y_oYalPZUSMp"
      },
      "cell_type": "markdown",
      "source": [
        "This project is particular to the fitness domain. To fine-tune a more capable** Mistral 7B** without encountering out-of-memory (OOM) errors on limited-capacity hardware, we loaded the large model, **Mistral 7B**, in 4  using bitsandbytes. More importantly, the QLoRA method was employed using the PEFT library from Hugging Face. This innovative technique updates only the essential low-rank approximations instead of the entire weight matrix, minimizing the memory requirements and enabling the use of less powerful GPUs for the fine-tuning process.\n",
        "\n",
        "The entire project was done on Google Collab Pro using the A100 GPU for less than 40 Euros\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Xmw9ssZnLYoj"
      },
      "cell_type": "markdown",
      "source": [
        "# **Loading packages and setting up the environment**"
      ]
    },
    {
      "metadata": {
        "id": "77LYAVCdO5rI"
      },
      "cell_type": "markdown",
      "source": [
        "To avoid UTF-8 errors when using the notebook for the project, the environment needs to be set up first. Also, restart the kennel after the \"accelerate -U\" installation pip to avoid further errors."
      ]
    },
    {
      "metadata": {
        "id": "xITwSQkf-RjY",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "%env LC_ALL=en_US.UTF-8\n",
        "%env LC_CTYPE=en_US.UTF-8\n",
        "%env LANG=en_US.UTF-8\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFaxkb47Nc3s",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers\n",
        "!pip install diffusers[\"torch\"]\n",
        "!pip install diffusers --upgrade\n",
        "!pip install -q wandb -U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0FnKuEYVN29g",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "pip install fastapi kaleido python-multipart uvicorn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jn-0ncfVOTdD",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import drive\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "nvmVLL_QV_aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb, os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "import shutil"
      ],
      "metadata": {
        "id": "O2ExXsqLWEA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8T5t_0SgLRyy"
      },
      "cell_type": "markdown",
      "source": [
        "# **Loading data**"
      ]
    },
    {
      "metadata": {
        "id": "atnN4iL3Xqww"
      },
      "cell_type": "markdown",
      "source": [
        "No preprocessing was actually required here since data was manually compiled into three columns: **main topic, sub-topic, and content**. A fourth column was also added, namely **'combined'**, to account for missing values if any in the main topic and sub-topic by combining them.\n",
        "\n",
        "The data was categorically collected in this manner for the purpose of our study. The intention is to generate content for the main- topic and sub-topic that are fed into the prompt. Not to confuse the AI, it makes it easier to collect data in such a structure for the purpose of this project. However, one can use traditional input and output like  ;\n",
        "\n",
        "{\"input\": \"Where is Ghana?\", \"output\": \"Ghana is in West Africa\"}\n",
        "\n",
        "{\"input\": \"What is the capital of Ghana?\", \"output\": \"Accra is the capital of Ghana\"}"
      ]
    },
    {
      "metadata": {
        "id": "mEky90n-v2qh",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# data source\n",
        "#https://www.acefitness.org/resources/pros/expert-articles/\n",
        "#https://www.healthline.com/fitness\n",
        "#https://www.bornfitness.com/category/fitness/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dy1kuy7TOeex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "04a5a68f0f3b41b0b0ae52ce832b8586",
            "36553b7b71fd40a2ad6cc00fa6c58a86",
            "6402e38116864e9cb4a7f27b9db2b38a",
            "09e6e274dcb8435bb85b19fbbe15680e",
            "f64a23bf573d463e925465ef41d9c664",
            "e0887f50c93241dc8bf1aa25b45d38f4",
            "b89aa0e4d2ae45ae94658fecb5ab2249",
            "98860d92bf914fd6a09c094886879853",
            "e18b8ee66a254a6aa750b7b624252336",
            "daad29d0773c46f59aa24f1a42fbe947",
            "284f7bcb7ec44a4eb94c67a4a12c5e38",
            "dce862ee42b24a8f929c41aea8018d7a",
            "77ad73bec4c34a4fae9c3ecdc40c84cb",
            "7c3873ae30324744951dbb09c24e8cba",
            "60bafb942748461ead2ba72b6e978df2",
            "f9a0d0e999294d71bdabc1f21a30ac56",
            "e05015c610b5492bad3180c7dcbeafa5",
            "592a3e5cd862489c939e72e7c3fd1e6c",
            "3abe5395267c42d7acd68565684cc744",
            "1f4518605dc54cdc9081662eef654343",
            "9ef7910ec3234b01a9ddf4b2d6eed0d9",
            "22ed9a4a6b9f405190e5df1c0579ea38"
          ]
        },
        "outputId": "8501f364-38b1-45db-9872-920320e25281",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset('json', data_files='train_set.jsonl', split='train')\n",
        "eval_dataset = load_dataset('json', data_files='test_set.jsonl', split='train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04a5a68f0f3b41b0b0ae52ce832b8586"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dce862ee42b24a8f929c41aea8018d7a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "8UrcoI8LkEqP"
      },
      "cell_type": "markdown",
      "source": [
        "### **Accelerator**"
      ]
    },
    {
      "metadata": {
        "id": "iKtVLyAeQV0G"
      },
      "cell_type": "markdown",
      "source": [
        "Step up an accelerator for training huge models on larger batch sizes, we can use a fully sharded data parallel model. This type of data parallel paradigm enables fitting more data and larger models by sharding the optimizer states, gradients, and parameters. It not entiely clear if this is needed for QLora but certainly has no adverse effect on the model."
      ]
    },
    {
      "metadata": {
        "id": "-SYuiChROsj0",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E72XY5TBRU6O"
      },
      "cell_type": "markdown",
      "source": [
        "Weights & Biases can be used to track our training metrics and system performance. The API can be applied for when prompted which is free and highly recommended when working on such models"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHz8-lQoXsb6",
        "outputId": "02fb51bc-cdbc-4442-bb8a-8a46ebee689d",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "\n",
        "wandb_project = \"journal-finetune\"\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myeboah-amoabeng\u001b[0m (\u001b[33myeboah\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        }
      ]
    },
    {
      "metadata": {
        "id": "NL_OL8L_ke8E"
      },
      "cell_type": "markdown",
      "source": [
        "### **Formatting prompts**"
      ]
    },
    {
      "metadata": {
        "id": "6hK1rE6IR-AB"
      },
      "cell_type": "markdown",
      "source": [
        "we then create a **formatting_func** as prompt to test drive the base model"
      ]
    },
    {
      "metadata": {
        "id": "od7XPqy4O-DE",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"### Write me a blog post about this topic: {example['content']}\"\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcNs7rQKSrWh"
      },
      "cell_type": "markdown",
      "source": [
        "Even though I am running the whole project in Google Collab Pro with an A100 GPU, I still deem it necessary to set up my device."
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpPRtXPrPccK",
        "outputId": "77621c6f-a35c-4ecf-84c0-d3f4a01eea2e",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "HbXydI3Qkv3c"
      },
      "cell_type": "markdown",
      "source": [
        "### **Load Base Model**"
      ]
    },
    {
      "metadata": {
        "id": "23mrLY_uTPXU"
      },
      "cell_type": "markdown",
      "source": [
        "we now load the based model, **Mistral-7B-v0.1** ,using 4-bit quantization."
      ]
    },
    {
      "metadata": {
        "id": "KqufqsYTPNCg",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ibc5wiJLk5bu"
      },
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "metadata": {
        "id": "4sQ6wrZUa7Rr"
      },
      "cell_type": "markdown",
      "source": [
        "To analyze the distribution of data lengths and optimize memory usage during training, it's essential to first tokenize your dataset without applying truncation or padding. This initial step allows you to understand the length distribution of your data. Once you have this distribution, you can set an appropriate **\"model_max_length\" **for your tokenizer. This value should be chosen based on the length distribution to efficiently handle the majority of your data without excessive padding or truncation."
      ]
    },
    {
      "metadata": {
        "id": "2wSEQ7LJPRhL",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_and_tokenize_prompt(prompt):\n",
        "    return tokenizer(formatting_func(prompt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMBlEPlLbi9v"
      },
      "cell_type": "markdown",
      "source": [
        "**Reformat the prompt and tokenize each sample.** To prepare your data for processing, begin by reformatting each sentence in the dataset. Following this, tokenize each reformatted sample. This approach involves two key steps: first, adjust the structure or format of each sentence to ensure consistency and suitability for your specific task; second, apply a tokenizer to each of these reformatted sentences. This process transforms your textual data into a format that is compatible with machine learning models, facilitating effective analysis or training."
      ]
    },
    {
      "metadata": {
        "id": "F_9YxgkiPZbn",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLC401yMdVHn"
      },
      "cell_type": "markdown",
      "source": [
        "Visualizing the length of the data"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "ivX6qdFkPoTj",
        "outputId": "f1e82407-1bba-40bb-91a1-3581b3b39a9c",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1890\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x600 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "uRQvTK4WcUX8"
      },
      "cell_type": "markdown",
      "source": [
        "The histogram of token lengths underscores the necessity of choosing an optimal max_length for our input tensors, which will be a balance between preserving information and computational efficiency. Given the skewed distribution towards shorter lengths, a judicious max_length can be selected that minimizes truncation while accommodating the majority of the data, thus ensuring minimal loss of content. This approach is reinforced by the careful preprocessing of the dataset, where samples were standardized in length by segmenting longer texts, with special care taken to not disrupt word or sentence integrity, thus facilitating a more efficient and coherent training process."
      ]
    },
    {
      "metadata": {
        "id": "8sSXMm_7dtmA"
      },
      "cell_type": "markdown",
      "source": [
        "For self-supervised fine-tuning, let's proceed with tokenization by applying both padding and truncation. This step ensures uniformity in token length across all samples. Additionally, configure the tokenization function to generate identical 'input_ids' and 'labels'. In self-supervised learning, this approach is common as it enables the model to predict the next token in a sequence or reconstruct the input, using its own outputs as training targets. This method of tokenization is a key component in preparing your data for self-supervised fine-tuning."
      ]
    },
    {
      "metadata": {
        "id": "gHaAeHkxP0hQ",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "max_length = 512 # This was an appropriate max length for my dataset\n",
        "\n",
        "def generate_and_tokenize_prompt2(prompt):\n",
        "    result = tokenizer(\n",
        "        formatting_func(prompt),\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EyluslTWP2Uv",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QD9y7aROe9Te"
      },
      "cell_type": "markdown",
      "source": [
        "Ensure that in your tokenization setup, input_ids are left-padded with the eos_token (typically denoted as 2), and also include an eos_token at the end of each sequence. Additionally, each sequence should begin with a bos_token (commonly represented as 1). This structure is crucial for maintaining consistency in sequence representation, where the bos_token marks the beginning, and eos_token signifies both padding and the end of the sequence. This setup is essential for training models effectively, particularly in language modeling tasks."
      ]
    },
    {
      "metadata": {
        "id": "raMT1Ob4P7mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd77dcb-370e-46ab-b275-bdd14add54a7",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "print(tokenized_train_dataset[1]['input_ids'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 12018, 528, 264, 6073, 1704, 684, 456, 9067, 28747, 23859, 272, 23966, 11317, 28725, 396, 330, 2554, 12089, 1799, 5926, 401, 7173, 560, 8621, 354, 5597, 28705, 28770, 28782, 1267, 304, 272, 18291, 304, 7162, 302, 401, 7173, 20114, 28725, 396, 5362, 304, 7731, 5403, 2496, 354, 2528, 14417, 28725, 17950, 18015, 28725, 21726, 1651, 1362, 2806, 304, 3327, 4154, 4799, 28723, 11317, 403, 5381, 297, 272, 500, 28723, 28735, 28723, 304, 2774, 516, 4835, 1312, 19735, 10539, 297, 9143, 28725, 970, 400, 2844, 5200, 390, 264, 11028, 10840, 361, 5377, 26558, 304, 3874, 516, 5968, 28742, 28713, 6153, 297, 6377, 9352, 28723, 650, 4142, 298, 5465, 28725, 4843, 297, 272, 28705, 28740, 28774, 28783, 28734, 28713, 390, 264, 330, 2554, 12089, 1799, 401, 7173, 16081, 28723, 1092, 369, 2723, 28809, 28707, 2066, 354, 11317, 28745, 400, 835, 4775, 272, 1820, 28725, 25659, 9970, 28747, 1094, 1529, 2482, 28809, 28713, 14555, 298, 18847, 2484, 8584, 297, 272, 5440, 401, 7173, 21268, 304, 3874, 272, 28705, 28750, 28734, 28734, 28787, 315, 1336, 28741, 401, 7173, 560, 8621, 302, 272, 7601, 8199, 28723, 2354, 23099, 304, 14900, 506, 4163, 713, 298, 4085, 297, 5597, 28705, 28782, 28734, 5780, 28725, 3227, 9696, 10437, 297, 754, 28705, 28740, 28784, 28734, 5611, 23311, 304, 8248, 380, 395, 9696, 9909, 356, 7028, 15245, 28723, 2387, 302, 516, 19225, 7028, 403, 11787, 330, 2554, 297, 10423, 871, 907, 5611, 4174, 297, 272, 28705, 28740, 28774, 28774, 28734, 28713, 28723, 28738, 19761, 28725, 11317, 349, 264, 4292, 302, 272, 9217, 302, 6055, 734, 302, 330, 2554, 28723, 816, 7310, 298, 713, 684, 516, 28705, 28770, 28782, 28733, 4395, 8123, 304, 516, 10195, 754, 272, 1267, 390, 264, 17950, 5024, 477, 5926, 401, 7173, 560, 8621, 298, 4779, 7059, 28723, 28705, 2]\n"
        }
      ]
    },
    {
      "metadata": {
        "id": "gVjewlPqfIrE"
      },
      "cell_type": "markdown",
      "source": [
        " visualization after applying padding and truncation"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "18L-EX32P_38",
        "outputId": "459ea336-190d-4831-d63c-9ce9a76e2829",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1890\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x600 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "PH9XDMTEh3wz"
      },
      "cell_type": "markdown",
      "source": [
        "After applying padding and truncation, all samples in your dataset should have a uniform length, specifically equal to max_length. This ensures consistency across the dataset, which is crucial for efficient processing and training in machine learning models, especially in language-related tasks. Uniform length is achieved by truncating longer samples and padding shorter ones to match the specified max_length."
      ]
    },
    {
      "metadata": {
        "id": "SIFyZ1UFlQIu"
      },
      "cell_type": "markdown",
      "source": [
        "## **How does the base model do?**"
      ]
    },
    {
      "metadata": {
        "id": "Ywzr0qBQiTOq"
      },
      "cell_type": "markdown",
      "source": [
        "Not particularly required but also necessary to test drive the model to check it initial performance."
      ]
    },
    {
      "metadata": {
        "id": "ZITLcmSZQGk7",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "eval_prompt = \" Write me a blog post about this topic: Fitness tips \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ8TvcmEQbkF",
        "outputId": "0c1c722b-b7d9-448b-f3c8-0168b42d1e26",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Init an eval tokenizer that doesn't add padding or eos token\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " Write me a blog post about this topic: Fitness tips 101\n\nIm not sure if you know, but I am a fitness instructor. I teach classes at the gym and I also have my own personal training business. I love helping people get fit and healthy! In this article, I will give you some of my best fitness tips for beginners. These are things that I tell all of my clients when they first start working with me. If you follow these simple steps, you will be on your way to reaching your fitness goals in no time!\n\n## What is fitness?\n\nFitness is the ability to do physical activity without being exhausted or injured. It includes endurance, strength, flexibility and balance. The more fit you are, the easier it will be for you to do everyday activities like walking up stairs or carrying groceries into your house. Being physically active can help prevent many diseases such as heart disease, diabetes and cancer.\n\n## Why should we care about our health?\n\nWe should care about our health because it affects every aspect of our lives. When we are healthy, we feel better emotionally and mentally which makes us happier overall. We also tend to live longer when we take good care of ourselves by eating right and exercising regularly.\n\n##\n"
        }
      ]
    },
    {
      "metadata": {
        "id": "jKReHuBxlfZF"
      },
      "cell_type": "markdown",
      "source": [
        "### **Set Up LoRA**"
      ]
    },
    {
      "metadata": {
        "id": "LSjTJhbfilzZ"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To begin fine-tuning your model, initiate with preprocessing using the 'prepare_model_for_kbit_training method from PEFT' (Prompt Engineering for Fine-Tuning). This method is specifically designed to condition the model for training by applying necessary modifications and optimizations in line with PEFT principles. It ensures that the model is aptly configured and ready for the efficient fine-tuning process."
      ]
    },
    {
      "metadata": {
        "id": "GJdttZakQtpM",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p_t-Qa-cQyWS",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QO5W-2uojElJ"
      },
      "cell_type": "markdown",
      "source": [
        "To prepare for applying QLoRA (Quantized Low-Rank Adaptation) to your model, start by printing the model's structure to examine its layers. Focus on identifying all the linear layers where QLoRA will be implemented. These layers typically include q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, and lm_head. Inspecting the model in this way allows you to understand its architecture and pinpoint where to apply QLoRA modifications effectively."
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER3h7c7EQ22F",
        "outputId": "7b5df3fa-1be1-4d05-f02f-cb91b41aa421",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=32, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=32, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)\n"
        }
      ]
    },
    {
      "metadata": {
        "id": "WhogRVbpk2CJ"
      },
      "cell_type": "markdown",
      "source": [
        "In defining the configuration for LoRA (Low-Rank Adaptation), we focus on two key parameters: r and alpha.\n",
        "\n",
        "r (Rank of Low-Rank Matrix): This parameter controls the number of parameters trained in the adapters. Essentially, it's the rank of the low-rank matrix used in LoRA. Choosing a higher rank increases the model's expressivity, as it allows for more complex adaptations. However, it also increases computational complexity. In the QLoRA paper, a rank of r=64 was used, but for our purposes, we'll set r=32. This choice strikes a balance between expressivity and computational efficiency, making the model more focused on the new, fine-tuned data while reducing computational load.\n",
        "\n",
        "lora_alpha (Scaling Factor for Learned Weights): The alpha parameter scales the learned weight matrix, with the scaling factor being alpha/r. A higher value of alpha gives more importance to the LoRA activations, thereby accentuating the influence of the fine-tuning. In the QLoRA paper, lora_alpha was set to 16, but we will use lora_alpha=64. This adjustment puts greater emphasis on the newly fine-tuned data, aligning with our goal of tailoring the model more distinctly to the new dataset.\n",
        "\n",
        "By setting r=32 and lora_alpha=64, we aim to optimize the model for significant adaptability to the new data while maintaining manageable computational requirements."
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCMpWP30Q6R0",
        "outputId": "2befa81f-0065-4de1-8da1-eeb0edc36e6a",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
        }
      ]
    },
    {
      "metadata": {
        "id": "dSzBE2SBmwWt"
      },
      "cell_type": "markdown",
      "source": [
        "Examine the modified structure of the model after integrating the LoRA (Low-Rank Adaptation) adapters. These additions will have altered the model's architecture, introducing changes that are visible when inspecting the model layers. Look for the implemented LoRA adapters to understand how they integrate with the existing structure, providing insight into the model's enhanced capabilities post-adaptation."
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFucyoYiQ_Ah",
        "outputId": "7ccac2be-f535-4aa0-d048-916709ac387e",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=32, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=32, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)\n"
        }
      ]
    },
    {
      "metadata": {
        "id": "2fVPqnFBlq3I"
      },
      "cell_type": "markdown",
      "source": [
        "### **Run Training!**"
      ]
    },
    {
      "metadata": {
        "id": "o2gCHeunpRLG"
      },
      "cell_type": "markdown",
      "source": [
        "In this training scenario, there is a relatively small dataset of about 1890 samples for both training and validation. Opting for 500 training steps, we were open to the possibility of overfitting, which in this context was acceptable. Overfitting occurs when a model, while improving on training data (decreasing training loss), performs poorly on new, unseen data (increasing validation loss). This typically indicates that the model is memorizing the training data rather than learning to generalize.\n",
        "\n",
        "In this case, overfitting was not a major concern as the goal was to fine-tune a model to generate outputs similar to blog posts. it was found that the end product worked well for this purpose, taking about 30 minutes on an A100 GPU.\n",
        "\n",
        "For training strategies:\n",
        "\n",
        "**Experiment with Max Steps:** Start with a high max_steps value. Monitor the model's performance and identify the point where overfitting starts, indicated by increasing validation loss and decreasing training loss. This point helps determine the optimal number of training steps.\n",
        "\n",
        "**Identify the Sweet Spot:** For example, if you start with 1000 steps and notice overfitting at around 500 steps, then 500 steps is likely your sweet spot for training. You would then use the model saved at this point (e.g., checkpoint-500 in your output directory) as the final model.\n",
        "\n",
        "**Explore Different Checkpoints:** You can experiment with models saved at different checkpoints to see varying degrees of overfitting and choose the one that best meets your needs. However, for the purpose of this study we allowed for complete training even though, our sweet spot was around checkpoint-150 where we could have interrupted the model\n",
        "\n",
        "**Interrupt if Necessary:** If you realize that sufficient training has been achieved before reaching your initially set max_steps, you can interrupt the training process to avoid unnecessary computation.\n",
        "\n",
        "This approach allows for flexibility and customization in training, especially when dealing with specific tasks like generating Fitness Blog Post."
      ]
    },
    {
      "metadata": {
        "id": "oyxhq00hRDBB",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z4hCj5GpRHzK",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "model = accelerator.prepare_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LAU93k2HRKpP",
        "outputId": "f6fdbc89-171e-416f-b46f-9a72e5ab177a",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "project = \"blog_post-finetune\"\n",
        "base_model_name = \"mistral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=500,\n",
        "        learning_rate= 2e-5, # Want a small lr for finetuning\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,              # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 25 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 25 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        report_to=\"wandb\",           # use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\" ,\n",
        "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "        metric_for_best_model=\"loss\",  # the metric for the best model\n",
        "        greater_is_better=False,  # Set to True if the metric should be maximized\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings.\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/content/wandb/run-20240203_150946-e2qu8q1u</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/yeboah/journal-finetune/runs/e2qu8q1u' target=\"_blank\">mistral-blog_post-finetune-2024-02-03-15-09</a></strong> to <a href='https://wandb.ai/yeboah/journal-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/yeboah/journal-finetune' target=\"_blank\">https://wandb.ai/yeboah/journal-finetune</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/yeboah/journal-finetune/runs/e2qu8q1u' target=\"_blank\">https://wandb.ai/yeboah/journal-finetune/runs/e2qu8q1u</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 1:10:44, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.947300</td>\n      <td>1.835919</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.835500</td>\n      <td>1.783581</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.735100</td>\n      <td>1.766186</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.717500</td>\n      <td>1.757314</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.679900</td>\n      <td>1.759988</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.593800</td>\n      <td>1.762661</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.538200</td>\n      <td>1.785924</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.487200</td>\n      <td>1.795691</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.448900</td>\n      <td>1.839452</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.376200</td>\n      <td>1.843180</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.327000</td>\n      <td>1.911269</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.272600</td>\n      <td>1.914001</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.239400</td>\n      <td>1.920638</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.151200</td>\n      <td>1.967255</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.161600</td>\n      <td>1.963684</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.073000</td>\n      <td>2.019707</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.070700</td>\n      <td>2.028916</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.043900</td>\n      <td>2.076888</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.004900</td>\n      <td>2.078729</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.009300</td>\n      <td>2.090130</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Checkpoint destination directory ./mistral-blog_post-finetune/checkpoint-25 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory ./mistral-blog_post-finetune/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory ./mistral-blog_post-finetune/checkpoint-75 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory ./mistral-blog_post-finetune/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory ./mistral-blog_post-finetune/checkpoint-125 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory ./mistral-blog_post-finetune/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:134: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=500, training_loss=1.3856618537902832, metrics={'train_runtime': 4256.2301, 'train_samples_per_second': 3.759, 'train_steps_per_second': 0.117, 'total_flos': 3.48311545542869e+17, 'train_loss': 1.3856618537902832, 'epoch': 9.26})"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "YyG4XttYtHD2"
      },
      "cell_type": "markdown",
      "source": [
        "# **Saving model onto drive**"
      ]
    },
    {
      "metadata": {
        "id": "3JmzhZMFooUD",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_path = '/content/mistral-blog_post-finetune'\n",
        "destination_path = '/content/drive/My Drive/NLG/mistral-blog_post-finetune'\n",
        "\n",
        "# Using shutil to copy the directory\n",
        "shutil.copytree(source_path, destination_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qTBThKsiwSBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe4f64d8-68e2-43b5-9f4d-075eaf739df9",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Define the source and destination paths\n",
        "source_path = '/content/wandb'\n",
        "destination_path = '/content/drive/My Drive/NLG/blog_wandb'\n",
        "\n",
        "# Using shutil to copy the directory\n",
        "shutil.copytree(source_path, destination_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/content/drive/My Drive/NLG/blog_wandb'",
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "DXSGSp5Yl3IW"
      },
      "cell_type": "markdown",
      "source": [
        "### **Trying the Trained Model!**"
      ]
    },
    {
      "metadata": {
        "id": "tQfUG5G4tVSD"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To manage memory usage effectively, it's advisable to terminate the current process, especially before loading a new model. This can be done by restarting the kernel (usually found under \"Kernel > Restart Kernel\" in most interfaces). This step prevents running out of memory that might occur if you attempt to load the base model on top of the already trained model in the same session.\n",
        "\n",
        "After restarting the kernel, you'll need to reload the base model. By default, the PEFT  library saves only the QLoRA adapters. Therefore, your next step should be to load the base model from the Huggingface Hub, onto which you can then integrate the saved QLoRA adapters. This process ensures you have the complete model setup, combining the original base model with the enhancements from the fine-tuning process."
      ]
    },
    {
      "metadata": {
        "id": "vcuJJPGShztL",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-jdO-CWtrZ8"
      },
      "cell_type": "markdown",
      "source": [
        "Next, proceed to load the QLoRA adapters from the checkpoint directory that corresponds to the best-performing model."
      ]
    },
    {
      "metadata": {
        "id": "63QR4mmvh9n3",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "ft_model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/NLG/mistral-blog_post-finetune/checkpoint-150\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHnMiKoSt8po"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can run multiple eval_prompt on our trained model to check if the fined-tuned model actually performs better. We just palyed around with a couple of eval_prompt."
      ]
    },
    {
      "metadata": {
        "id": "gl6_ynwLY5Yd",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#def formatting_func(example):\n",
        "   # text = f\"### Write me a blog post about this topic: {example}. Divide your post into subtopic and write several paragraphs for each subtopic\"\n",
        "    #return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lqy2TpnZiMjC",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#eval_prompt = \"Write me a blog post about this topic: fitness tips  \"\n",
        "#model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "#ft_model.eval()\n",
        "#with torch.no_grad():\n",
        "    #print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=1000, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5767be-a509-4ef5-fca6-10b93d72e4d6",
        "trusted": false,
        "id": "PliG3dz0Vst_"
      },
      "cell_type": "code",
      "source": [
        "#tryout 1\n",
        "# Define your main topic, subtopics, and key SEO keywords\n",
        "main_topic = \"Revolutionary Fitness Program for the Modern Era\"\n",
        "\n",
        "subtopics = [\n",
        "    \"Integration of Technology in Fitness\",\n",
        "    \"Personalized Workout Routines for Every Individual\",\n",
        "    \"Mental Health and Fitness\",\n",
        "    \"Balancing Modern Life with Physical Activity\",\n",
        "    \"Community and Support in Fitness\"\n",
        "]\n",
        "seo_keywords = [\n",
        "    \"technology in fitness\",\n",
        "    \"personalized fitness plans\",\n",
        "    \"mental health\",\n",
        "    \"physical activity\",\n",
        "    \"fitness community\",\n",
        "    \"modern fitness trends\",\n",
        "    \"health and wellness\",\n",
        "    \"active lifestyle\"\n",
        "]\n",
        "\n",
        "# Combine the main topic, subtopics, and SEO keywords into the prompt\n",
        "eval_prompt = f\"Write me a blog post about this topic: {main_topic}, including subtopics: {' | '.join(subtopics)} and ensure to integrate these key SEO keywords: {' | '.join(seo_keywords)}  in a well-formatted and structured paragraphs.\"\n",
        "\n",
        "# Tokenize the prompt using eval_tokenizer\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ft_model.eval()\n",
        "\n",
        "# Generate content based on the model input\n",
        "with torch.no_grad():\n",
        "    generated_tokens = ft_model.generate(\n",
        "        **model_input,\n",
        "        max_new_tokens=1000,\n",
        "        repetition_penalty=1.15\n",
        "    )\n",
        "\n",
        "# Decode and print the generated content\n",
        "generated_text = eval_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write me a blog post about this topic: Revolutionary Fitness Program for the Modern Era, including subtopics: Integration of Technology in Fitness | Personalized Workout Routines for Every Individual | Mental Health and Fitness | Balancing Modern Life with Physical Activity | Community and Support in Fitness and ensure to integrate these key SEO keywords: technology in fitness | personalized fitness plans | mental health | physical activity | fitness community | modern fitness trends | health and wellness | active lifestyle  in a well-formatted and structured paragraphs.\n",
            "\n",
            "The fitness industry has undergone significant changes over the years, adapting to the evolving needs and preferences of individuals seeking to improve their health and well-being. In recent times, we have witnessed a shift towards more personalized approaches that cater to the unique requirements of each individual. This article will explore some of the most prominent modern fitness trends, highlighting how they are revolutionizing the way people approach exercise and overall wellness.\n",
            "\n",
            "Integration of Technology in Fitness\n",
            "Technology has become an integral part of our lives, and it is no different when it comes to fitness. The integration of technology into fitness programs allows individuals to track their progress, monitor their heart rate, and receive real-time feedback on their performance. Wearable devices such as smartwatches and fitness trackers provide valuable insights into metrics like steps taken, calories burned, sleep quality, and even stress levels. These data points can help individuals make informed decisions about their workouts, adjust their routines accordingly, and stay motivated by tracking their progress over time.\n",
            "\n",
            "Personalized Workout Routines for Every Individual\n",
            "One-size-fits-all workout plans are becoming obsolete as fitness professionals recognize the importance of tailoring exercises to meet the specific needs and goals of each client. Personal trainers now conduct comprehensive assessments to identify areas of strength, weakness, imbalances, and any existing injuries or medical conditions. Based on this information, customized workout routines are designed to address individual limitations while challenging the body in ways that promote growth and improvement. By focusing on personalization, individuals can experience greater success, reduced risk of injury, and enhanced overall satisfaction with their fitness journey.\n",
            "\n",
            "Mental Health and Fitness\n",
            "In todays fast-paced world, mental well-being has gained prominence as an essential component of overall health. Recognizing the interconnectedness between physical activity and mental health, many fitness enthusiasts are incorporating mindfulness practices into their workout routines. Activities such as yoga, meditation, and breathwork not only enhance flexibility and balance but also promote relaxation, reduce stress, and improve cognitive function. Additionally, group fitness classes offer a supportive environment where participants can connect with others, build camaraderie, and find solace through shared experiences.\n",
            "\n",
            "Balancing Modern Life with Physical Activity\n",
            "With the rise of sedentary lifestyles due to desk jobs and increased screen time, finding opportunities to incorporate physical activity into daily routines has become crucial. To combat this challenge, innovative solutions have emerged, allowing individuals to seamlessly integrate exercise into their busy schedules. High-intensity interval training (HIIT) workouts, which involve short bursts of intense effort followed by periods of rest, have gained popularity due to their efficiency and effectiveness. HIIT sessions can be completed in as little as 15 minutes, making them ideal for those with limited time. Furthermore, the availability of online fitness platforms and virtual coaching services enables individuals to access expert guidance and workout resources from anywhere, eliminating geographical barriers and facilitating convenient access to fitness programming.\n",
            "\n",
            "Community and Support in Fitness\n",
            "Fitness communities have evolved beyond traditional gym settings, fostering a sense of belonging and support among like-minded individuals. Social media platforms serve as powerful tools for connecting with others who share similar interests and goals. Online fitness challenges, group workout sessions, and motivational posts create a supportive environment where individuals can encourage one another, celebrate achievements, and seek advice from experienced peers. Additionally, specialized fitness studios and boutique gyms offer intimate class environments, providing a sense of camaraderie and connection that enhances the overall fitness experience.\n",
            "\n",
            "Conclusion\n",
            "As the fitness landscape continues to evolve, it becomes increasingly clear that modern trends are reshaping the way we approach exercise and wellness. From the integration of technology to personalized workout routines, mental health awareness, balancing life with physical activity, and fostering strong communities, these trends emphasize the importance of tailored approaches that prioritize the unique needs and aspirations of each individual. As we embrace these innovations, we can expect continued advancements in the field of fitness, leading to improved health outcomes and a heightened focus on holistic well-being.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5lCJTL_J8kMm"
      },
      "cell_type": "markdown",
      "source": [
        "**Saving generated blog into a file, to be used later for image generation**"
      ]
    },
    {
      "metadata": {
        "id": "mFci9sDq8k7w",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Using shutil to copy the directory\n",
        "shutil.copytree(source_path, destination_path)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "\n",
        "# Now save the file\n",
        "with open('/content/data/generated_blog_post.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(generated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztnYxndp9QJF",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "with open('/content/data/generated_blog_post.txt', 'r', encoding='utf-8') as file:\n",
        "    generated_blog_post = file.read()\n",
        "\n",
        "print(generated_blog_post)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QhokQGZBKAuu"
      },
      "cell_type": "markdown",
      "source": [
        "**This part is just a repetition for the text generation but also necessary to proof check your output**"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c332df3f-a605-436f-cdfa-f8fb2fa3c991",
        "trusted": false,
        "id": "gnDk-Eq4V2cZ"
      },
      "cell_type": "code",
      "source": [
        "#try out 2\n",
        "# Define your main topic, subtopics, and key SEO keywords\n",
        "main_topic = \"Fostering Mental Health Through Fitness\"\n",
        "subtopics = [\n",
        "    \"Mind-Body Exercise Regimens\",\n",
        "    \"Nutritional Wellness for Mental Clarity\",\n",
        "    \"Community and Group Fitness Dynamics\",\n",
        "    \"Restorative Environments for Recovery\"\n",
        "]\n",
        "seo_keywords = [\n",
        "  \"mental health awareness\",\n",
        "  \"social connection\",\n",
        "  \"mind-body connection\"\n",
        "]\n",
        "\n",
        "# Combine the main topic, subtopics, and SEO keywords into the prompt\n",
        "eval_prompt = f\"Write me a blog post about this topic: {main_topic}, including subtopics: {' | '.join(subtopics)} and ensure to integrate these key SEO keywords: {' | '.join(seo_keywords)}  in a well-formatted and structured paragraphs.\"\n",
        "\n",
        "# Tokenize the prompt using eval_tokenizer\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "ft_model.eval()\n",
        "\n",
        "# Generate content based on the model input\n",
        "with torch.no_grad():\n",
        "    generated_tokens = ft_model.generate(\n",
        "        **model_input,\n",
        "        max_new_tokens=1000,\n",
        "        repetition_penalty=1.15\n",
        "    )\n",
        "\n",
        "# Decode and print the generated content\n",
        "generated_text = eval_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write me a blog post about this topic: Fostering Mental Health Through Fitness, including subtopics: Mind-Body Exercise Regimens | Nutritional Wellness for Mental Clarity | Community and Group Fitness Dynamics | Restorative Environments for Recovery and ensure to integrate these key SEO keywords: mental health awareness | social connection | mind-body connection  in a well-formatted and structured paragraphs.\n",
            "\n",
            "Fostering Mental Health Through Fitness\n",
            "\n",
            "The COVID-19 pandemic has had an unprecedented impact on the physical and mental health of people around the world. As we continue to navigate through this challenging time, it is essential to prioritize our overall well-being by incorporating activities that promote both physical fitness and mental health. In this article, we will explore how fitness can play a vital role in fostering mental health and provide practical tips for integrating exercise into your daily routine.\n",
            "\n",
            "Mind-Body Exercise Regimens\n",
            "\n",
            "Engaging in mind-body exercises such as yoga or tai chi can be incredibly beneficial for promoting mental well-being. These practices involve synchronizing movement with breath, which helps to calm the mind, reduce stress, and improve focus. Additionally, the social aspect of participating in group classes can foster a sense of community and belonging, further enhancing mental health.\n",
            "\n",
            "Nutritional Wellness for Mental Clarity\n",
            "\n",
            "Proper nutrition plays a crucial role in maintaining both physical and mental health. Eating a balanced diet rich in fruits, vegetables, whole grains, lean proteins, and healthy fats can help regulate mood, increase energy levels, and support cognitive function. Its important to consult with a healthcare professional or registered dietitian to develop a personalized meal plan tailored to individual needs and preferences.\n",
            "\n",
            "Community and Group Fitness Dynamics\n",
            "\n",
            "Participating in group fitness programs not only provides opportunities for physical activity but also offers social benefits that contribute to mental health. Engaging in team sports, joining running clubs, or attending group exercise classes allows individuals to connect with others who share similar interests and goals. This sense of camaraderie and shared experience can significantly enhance feelings of belonging and self-worth.\n",
            "\n",
            "Restorative Environments for Recovery\n",
            "\n",
            "Creating restorative environments within fitness facilities can positively impact mental health. Design elements such as calming colors, natural lighting, and soothing music can help create a relaxing atmosphere conducive to recovery after workouts. Additionally, offering amenities like saunas, steam rooms, or massage therapy can provide additional opportunities for relaxation and stress relief.\n",
            "\n",
            "Conclusion\n",
            "\n",
            "Incorporating regular physical activity into ones lifestyle is essential for maintaining both physical and mental health. By engaging in mind-body exercises, focusing on nutritional wellness, participating in group fitness programs, and creating restorative environments, individuals can effectively manage their mental well-being while enjoying the numerous benefits of staying active. Prioritizing mental health through fitness is not only beneficial for oneself but also contributes to building stronger communities and a healthier society as a whole.\n",
            "\n",
            "SEO Keywords: mental health awareness | social connection | mind-body connection\n",
            "\n",
            "Adam Bornstein is a New York Times bestselling author and the author of You Cant Screw This Up. He is the founder of Born Fitness, and the co-founder of Arnolds Pump Club (with Arnold Schwarzenegger) and Pen Name Consulting. An award-winning writer and editor, Bornstein was previously the Chief Nutrition Officer for Ladder, the Fitness and Nutrition editor for Mens Health, Editorial Director at LIVESTRONG.com, and a columnist for SHAPE, Mens Fitness, and Muscle & Fitness. Hes also a nutrition and fitness advisor for LeBron James, Cindy Crawford, Lindsey Vonn, and Arnold Schwarzenegger. According to The Huffington Post, Bornstein is one of the most inspiring sources in all of health and fitness. His work has been featured in dozens of publications, including The New York Times, Fast Company, ESPN, and GQ, and hes appeared on Good Morning America, The Today Show, and E! News. He lives in Los Angeles with his wife and two children. The views expressed in this article are the authors own.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MA4_75Bp-vx8",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Now save the file\n",
        "with open('/content/data/generated_blog_post2.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(generated_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNmyZZM+1KD84AMeMCK6lvp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04a5a68f0f3b41b0b0ae52ce832b8586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36553b7b71fd40a2ad6cc00fa6c58a86",
              "IPY_MODEL_6402e38116864e9cb4a7f27b9db2b38a",
              "IPY_MODEL_09e6e274dcb8435bb85b19fbbe15680e"
            ],
            "layout": "IPY_MODEL_f64a23bf573d463e925465ef41d9c664",
            "tabbable": null,
            "tooltip": null
          }
        },
        "36553b7b71fd40a2ad6cc00fa6c58a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e0887f50c93241dc8bf1aa25b45d38f4",
            "placeholder": "",
            "style": "IPY_MODEL_b89aa0e4d2ae45ae94658fecb5ab2249",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating train split: "
          }
        },
        "6402e38116864e9cb4a7f27b9db2b38a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_98860d92bf914fd6a09c094886879853",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e18b8ee66a254a6aa750b7b624252336",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "09e6e274dcb8435bb85b19fbbe15680e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_daad29d0773c46f59aa24f1a42fbe947",
            "placeholder": "",
            "style": "IPY_MODEL_284f7bcb7ec44a4eb94c67a4a12c5e38",
            "tabbable": null,
            "tooltip": null,
            "value": " 1701/0 [00:00&lt;00:00, 48646.94 examples/s]"
          }
        },
        "f64a23bf573d463e925465ef41d9c664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0887f50c93241dc8bf1aa25b45d38f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89aa0e4d2ae45ae94658fecb5ab2249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "98860d92bf914fd6a09c094886879853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e18b8ee66a254a6aa750b7b624252336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "daad29d0773c46f59aa24f1a42fbe947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "284f7bcb7ec44a4eb94c67a4a12c5e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "dce862ee42b24a8f929c41aea8018d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77ad73bec4c34a4fae9c3ecdc40c84cb",
              "IPY_MODEL_7c3873ae30324744951dbb09c24e8cba",
              "IPY_MODEL_60bafb942748461ead2ba72b6e978df2"
            ],
            "layout": "IPY_MODEL_f9a0d0e999294d71bdabc1f21a30ac56",
            "tabbable": null,
            "tooltip": null
          }
        },
        "77ad73bec4c34a4fae9c3ecdc40c84cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e05015c610b5492bad3180c7dcbeafa5",
            "placeholder": "",
            "style": "IPY_MODEL_592a3e5cd862489c939e72e7c3fd1e6c",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating train split: "
          }
        },
        "7c3873ae30324744951dbb09c24e8cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_3abe5395267c42d7acd68565684cc744",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f4518605dc54cdc9081662eef654343",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "60bafb942748461ead2ba72b6e978df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_9ef7910ec3234b01a9ddf4b2d6eed0d9",
            "placeholder": "",
            "style": "IPY_MODEL_22ed9a4a6b9f405190e5df1c0579ea38",
            "tabbable": null,
            "tooltip": null,
            "value": " 189/0 [00:00&lt;00:00, 10455.33 examples/s]"
          }
        },
        "f9a0d0e999294d71bdabc1f21a30ac56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e05015c610b5492bad3180c7dcbeafa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592a3e5cd862489c939e72e7c3fd1e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "3abe5395267c42d7acd68565684cc744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1f4518605dc54cdc9081662eef654343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ef7910ec3234b01a9ddf4b2d6eed0d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ed9a4a6b9f405190e5df1c0579ea38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}